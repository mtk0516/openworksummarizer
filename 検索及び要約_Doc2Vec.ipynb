{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d340d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.charfilter import UnicodeNormalizeCharFilter, RegexReplaceCharFilter\n",
    "from janome.tokenizer import Tokenizer as JanomeTokenizer  # sumyのTokenizerと名前が被るため\n",
    "from janome.tokenfilter import POSKeepFilter, ExtractAttributeFilter\n",
    "import re\n",
    "import ipadic\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "#Algos\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.reduction import ReductionSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.sum_basic import SumBasicSummarizer\n",
    "from sumy.summarizers.kl import KLSummarizer\n",
    "from janome.tokenizer import Tokenizer as JanomeTokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.charfilter import *\n",
    "from janome.tokenfilter import *\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "import pkg_resources, imp\n",
    "imp.reload(pkg_resources)\n",
    "import unicodedata\n",
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')#spacyの日本語モデル(Ginza)をロード\n",
    "\n",
    "from ginza import *\n",
    "import neologdn\n",
    "import re\n",
    "import emoji\n",
    "import mojimoji\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd084fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "#\n",
    "import sqlite3\n",
    "# ------------------------------------------------------------------\n",
    "sys.stderr.write(\"*** 開始 ***\\n\")\n",
    "#読み込み先\n",
    "file_company = \"./datas/OpenWork_company.db\"\n",
    "file_company_info = './datas/OpenWork_company_info.db'\n",
    "\n",
    "conn = sqlite3.connect(file_company)\n",
    "conn2 = sqlite3.connect(file_company_info)\n",
    "#\n",
    "company_df=pd.read_sql_query('SELECT * FROM df_company', conn)\n",
    "company_info_df=pd.read_sql_query('SELECT * FROM df_company_info', conn2)\n",
    "display(company_df.head(15))\n",
    "display(company_info_df.head(15))\n",
    "#\n",
    "conn.close()\n",
    "conn2.close()\n",
    "sys.stderr.write(\"*** 終了 ***\\n\")\n",
    "\n",
    "#複数行に値が重複している可能性があるためdrop_duplicatesで、1行1ユニーク値にする\n",
    "company_df = company_df.drop_duplicates()\n",
    "company_info_df = company_info_df.drop_duplicates()\n",
    "\n",
    "company_info_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4aa2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#company_dfの前処理\n",
    "company_df.本文 = company_df.本文.str.replace('\\u3000', ' ')\n",
    "company_df.本文 = company_df.本文.str.replace('■', ' ')\n",
    "company_df.本文 = company_df.本文.str.replace('⇒', ' ')\n",
    "company_df.本文 = company_df.本文.str.replace('→', ' ')\n",
    "company_df.本文 = company_df.本文.str.replace('□', ' ')\n",
    "company_df.本文 = company_df.本文.str.replace('：', ' ')\n",
    "company_df.本文 = company_df.本文.str.replace('▷', ' ')\n",
    "#company_info_dfの前処理\n",
    "def preprocessing(company_info_df):\n",
    "        \n",
    "    #まず、'--'が含まれている場合はNaNに置き換える\n",
    "    replace_line = company_info_df.replace('--',None)\n",
    "\n",
    "    #欠損していない値が3未満の企業はdrop\n",
    "    processed_company_info_df = replace_line.dropna(axis=1, thresh=3,  inplace=False)\n",
    "\n",
    "    #obj→floatに変換\n",
    "    processed_company_info_df.月残業時間 = processed_company_info_df.月残業時間.astype('float')\n",
    "    processed_company_info_df.有給消化率 = processed_company_info_df.有給消化率.astype('float')\n",
    "    processed_company_info_df.平均年収  = processed_company_info_df.平均年収.astype('float')\n",
    "    processed_company_info_df.総合スコア = processed_company_info_df.総合スコア.astype('float')\n",
    "    processed_company_info_df.社員の士気 = processed_company_info_df.社員の士気.astype('float')\n",
    "    processed_company_info_df.風通し = processed_company_info_df.風通し.astype('float')\n",
    "    processed_company_info_df.社員相互尊重 = processed_company_info_df.社員相互尊重.astype('float')\n",
    "    processed_company_info_df['20代成長性'] = processed_company_info_df['20代成長性'].astype('float')\n",
    "    processed_company_info_df.長期育成 = processed_company_info_df.長期育成.astype('float')\n",
    "    processed_company_info_df.コンプラ = processed_company_info_df.コンプラ.astype('float')\n",
    "    processed_company_info_df.評価納得感 = processed_company_info_df.評価納得感.astype('float')\n",
    "    processed_company_info_df.待遇満足度 = processed_company_info_df.待遇満足度.astype('float')\n",
    "    \n",
    "    #Nullは中央値で埋める\n",
    "    processed_company_info_df = processed_company_info_df.fillna(value=processed_company_info_df.median())\n",
    "    \n",
    "    #その他、分析に必要なdfの必要な前処理\n",
    "    processed_company_info_df['会社年齢'] = 2022 - processed_company_info_df['設立年']\n",
    "    processed_company_info_df.drop('設立年',axis=1,inplace=True)\n",
    "    \n",
    "    return processed_company_info_df\n",
    "\n",
    "company_info_df_processed = preprocessing(company_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1652cfa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword list \n",
    "stopword_list = []\n",
    "with open('./stopwords/stopwords.csv','r',encoding='utf-8') as f:\n",
    "    text = f.readlines()\n",
    "    for tk in text:\n",
    "        tk = tk.strip('\\n')\n",
    "        tk = tk.replace('\\ufeff','')\n",
    "        stopword_list.append(tk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67b8cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#指定した企業IDの企業コメントを可視化し、要約する関数\n",
    "\n",
    "#企業ID指定\n",
    "company_name = 'グーグル合同会社'\n",
    "\n",
    "#algos\n",
    "\n",
    "algorithm_dic = {\"lex\": LexRankSummarizer(), \"tex\": TextRankSummarizer(), \"lsa\": LsaSummarizer(),\\\n",
    "                 \"kl\": KLSummarizer(), \"luhn\": LuhnSummarizer(), \"redu\": ReductionSummarizer(),\\\n",
    "                 \"sum\": SumBasicSummarizer()}\n",
    "\n",
    "#日本語処理用のクラス\n",
    "class JapaneseCorpus:\n",
    "    # ①\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('ja_ginza')\n",
    "        self.analyzer = Analyzer(\n",
    "            char_filters=[UnicodeNormalizeCharFilter(), RegexReplaceCharFilter(r'[(\\)「」、。]', ' ')],  # ()「」、。は全てスペースに置き換える\n",
    "            tokenizer=JanomeTokenizer(),\n",
    "            token_filters=[POSKeepFilter(['名詞', '形容詞', '副詞', '動詞']), ExtractAttributeFilter('base_form')]  # 名詞・形容詞・副詞・動詞の原型のみ\n",
    "        )\n",
    "\n",
    "    # ②\n",
    "    def preprocessing(self, text):#前処理の関数\n",
    "        text = re.sub(r'\\n', '', text)\n",
    "        text = re.sub(r'\\r', '', text)\n",
    "        #text = re.sub(r'\\d', '', text)\n",
    "        text = re.sub(r'\\s', '', text)\n",
    "        text.replace('\\u3000', '')\n",
    "        text = text.lower()\n",
    "        text = mojimoji.zen_to_han(text, kana=True)\n",
    "        text = mojimoji.han_to_zen(text, digit=False, ascii=False)\n",
    "        text = ''.join(c for c in text if c not in emoji.UNICODE_EMOJI)\n",
    "        text = neologdn.normalize(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    # ③\n",
    "    def make_sentence_list(self, sentences):\n",
    "        doc = self.nlp(sentences)\n",
    "        self.ginza_sents_object = doc.sents #与えられた文字列を文章単位に分割\n",
    "        sentence_list = [s for s in doc.sents]\n",
    "\n",
    "        return sentence_list\n",
    "\n",
    "    # ④\n",
    "    def make_corpus(self):\n",
    "        corpus = [' '.join(self.analyzer.analyze(str(s))) + '。' for s in self.ginza_sents_object]\n",
    "\n",
    "        return corpus\n",
    "\n",
    "\n",
    "#WordCloudで可視化と、レビューの要約双方を行うクラス\n",
    "class company_summarize:\n",
    "    def __init__(self,company_name):\n",
    "        self.company_name = company_name\n",
    "        self.company_id = company_df[company_df.企業名 == company_name].企業ID.unique()[0]#compane_nameをIDに変換\n",
    "        self.nlp = spacy.load('ja_ginza')\n",
    "        self.analyzer = Analyzer(\n",
    "            char_filters=[UnicodeNormalizeCharFilter(), RegexReplaceCharFilter(r'[(\\)「」、。]', ' ')],  # ()「」、。は全てスペースに置き換える\n",
    "            tokenizer=JanomeTokenizer(),\n",
    "            token_filters=[POSKeepFilter(['名詞', '形容詞', '副詞', '動詞']), ExtractAttributeFilter('base_form')]  # 名詞・形容詞・副詞・動詞の原型のみ\n",
    "        )\n",
    "\n",
    "    \n",
    "    #Wordcloudでの可視化\n",
    "    def visualize(self):\n",
    "        CHASEN_ARGS = r' -F \"%m\\t%f[7]\\t%f[6]\\t%F-[0,1,2,3]\\t%f[4]\\t%f[5]\\n\"'\n",
    "        CHASEN_ARGS += r' -U \"%m\\t%m\\t%m\\t%F-[0,1,2,3]\\t\\t\\n\"'\n",
    "        tagger = MeCab.Tagger(ipadic.MECAB_ARGS + CHASEN_ARGS)\n",
    "        #null parse\n",
    "        tagger.parse('')\n",
    "    \n",
    "        #指定した企業IDのみに絞る\n",
    "        company_text = pd.DataFrame(company_df[company_df.企業名 == self.company_name])\n",
    "\n",
    "        #dfの本文から口コミデータを抽出し、strで連結しテキストデータへ\n",
    "        all_text = company_text.本文.str.cat()\n",
    "    \n",
    "        #正規化\n",
    "        all_text_norm = unicodedata.normalize(\"NFKC\", all_text)\n",
    "    \n",
    "        #parse\n",
    "        node = tagger.parseToNode(all_text_norm)\n",
    "\n",
    "        # 指定した品詞を抽出しリストに\n",
    "        word_list = []\n",
    "    \n",
    "        while node:\n",
    "            word_type = node.feature.split(',')[0]\n",
    "            if word_type in [\"名詞\", \"動詞\", \"形容詞\",'副詞']:\n",
    "                word_list.append(node.surface)\n",
    "            #ここのインデントミスると無限ループ\n",
    "            node = node.next\n",
    "\n",
    "        # リストを文字列に変換\n",
    "        word_chain = ' '.join(word_list)\n",
    "    \n",
    "        #ストップワードリストをコピー\n",
    "        stopword_list_indef = stopword_list\n",
    "        \n",
    "        #word_listの要素数の10%は、最も登場頻度が高いワードなのでストップリストに追加する\n",
    "        fdist = Counter(word_list)\n",
    "        Common_Words = fdist.most_common(n=int(len(fdist)*0.10))\n",
    "        \n",
    "        for common_word in Common_Words:\n",
    "            stopword_list_indef.append(common_word[0])\n",
    "\n",
    "        #社名が入らないように、stopwordlistに追加\n",
    "        stopword_list_indef.append(company_text.企業名.drop_duplicates().str.cat().replace('株式会社',''))\n",
    "\n",
    "        # ワードクラウド作成\n",
    "        W = WordCloud(width=640, height=480, background_color='black', colormap='cool_r',\n",
    "                      font_path='C:\\Windows\\Fonts\\yumin.ttf', stopwords = set(stopword_list_indef)).generate(word_chain)\n",
    "        # 表示設定\n",
    "        plt.figure(figsize = (15, 12))\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.imshow(W)\n",
    "        \n",
    "    #前処理、コーパスリスト作成など\n",
    "    def preprocessing(self, text):\n",
    "        text = re.sub(r'\\n', '', text)\n",
    "        text = re.sub(r'\\r', '', text)\n",
    "        #text = re.sub(r'\\d', '', text)\n",
    "        text = re.sub(r'\\s', '', text)\n",
    "        text.replace('\\u3000', '')\n",
    "        text = text.lower()\n",
    "        text = mojimoji.zen_to_han(text, kana=True)\n",
    "        text = mojimoji.han_to_zen(text, digit=False, ascii=False)\n",
    "        text = ''.join(c for c in text if c not in emoji.UNICODE_EMOJI)\n",
    "        text = neologdn.normalize(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def make_sentence_list(self, sentences):\n",
    "        doc = self.nlp(sentences)\n",
    "        self.ginza_sents_object = doc.sents\n",
    "        sentence_list = [s for s in doc.sents]\n",
    "\n",
    "        return sentence_list\n",
    "\n",
    "    def make_corpus(self):\n",
    "        corpus = [' '.join(self.analyzer.analyze(str(s))) + '。' for s in self.ginza_sents_object]\n",
    "\n",
    "        return corpus\n",
    "    \n",
    "    #文章内容要約\n",
    "    def summarize_sentences(self,sentences_count=10, algorithm=\"lex\", language=\"japanese\"):\n",
    "        \n",
    "        company_split_df = pd.DataFrame(company_df[company_df.企業名 == self.company_name])\n",
    "        if company_split_df.shape[0] >= 55:\n",
    "            company_split_df = company_split_df.sample(n=55)\n",
    "        #任意の企業名のレビューstrを取り出す\n",
    "        all_text = company_split_df.本文.str.cat()\n",
    "        all_text_norm = unicodedata.normalize(\"NFKC\", all_text)\n",
    "        sentences=' '.join(all_text_norm)\n",
    "        \n",
    "        #JPコーパスクラス継承\n",
    "        corpus_maker = JapaneseCorpus()\n",
    "        preprocessed_sentences = corpus_maker.preprocessing(sentences)\n",
    "        preprocessed_sentence_list = corpus_maker.make_sentence_list(preprocessed_sentences)\n",
    "        corpus = corpus_maker.make_corpus()\n",
    "        parser = PlaintextParser.from_string(\" \".join(corpus), Tokenizer(language))\n",
    "\n",
    "        try:\n",
    "            summarizer = algorithm_dic[algorithm]\n",
    "        except KeyError:\n",
    "            print(\"algorithm name:'{}'is not found.\".format(algorithm))\n",
    "\n",
    "        summarizer.stop_words = get_stop_words(language)\n",
    "        #sentences_countは文書に一定の割合をかけた値でもよいが、読みやすさ重視で10センテンス固定長とする\n",
    "        #int(len(corpus)*0.1)\n",
    "        summary = summarizer(document=parser.document, sentences_count=10)\n",
    "        \n",
    "        \n",
    "        if language == \"japanese\":\n",
    "            return str(\"\".join([str(preprocessed_sentence_list[corpus.index(sentence.__str__())]) for sentence in summary]))\n",
    "        else:\n",
    "            return str(\"\".join([sentence.__str__() for sentence in summary]))\n",
    "\n",
    "#run\n",
    "if __name__ == '__main__':\n",
    "    C = company_summarize(company_name)\n",
    "    \n",
    "    #結果の表示\n",
    "    print('\\033[1m' + C.company_name + '\\033[0m'+ '\\n')\n",
    "    #company_info_dfから該当企業のスコアを表示\n",
    "    print('\\033[1m' + '該当企業の分野別評価' + '\\033[0m'+ '\\n')\n",
    "    display(company_info_df_processed[company_info_df_processed.企業名 == company_name].drop('業種',axis=1))\n",
    "    #該当企業の業種の平均スコアを表示\n",
    "    print('\\033[1m' + '該当企業の業種平均評価' + '\\033[0m' + '\\n')\n",
    "    \n",
    "    #業種平均を表示\n",
    "    company_info_mean = company_info_df_processed.groupby('業種').agg(np.mean).round(2)\n",
    "    display(company_info_mean[company_info_mean.index == company_info_df[company_info_df.企業名 == company_name].業種.str.cat()])\n",
    "\n",
    "    print('------------------------')\n",
    "    #WordCloudを表示する\n",
    "    C.visualize()\n",
    "\n",
    "    #文書要約を表示 読みやすさ重視で10センテンス固定長とする\n",
    "    print(C.summarize_sentences(sentences_count=10, algorithm=\"luhn\", language=\"japanese\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73926ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc25084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12309f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thanks to https://zenn.dev/shikumiya_hata/articles/b18e362e2eae09\n",
    "#内容要約\n",
    "#前処理を集約したクラス\n",
    "class JapaneseCorpus:\n",
    "    # ①\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('ja_ginza')\n",
    "        self.analyzer = Analyzer(\n",
    "            char_filters=[UnicodeNormalizeCharFilter(), RegexReplaceCharFilter(r'[(\\)「」、。]', ' ')],  # ()「」、。は全てスペースに置き換える\n",
    "            tokenizer=JanomeTokenizer(),\n",
    "            token_filters=[POSKeepFilter(['名詞', '形容詞', '副詞', '動詞']), ExtractAttributeFilter('base_form')]  # 名詞・形容詞・副詞・動詞の原型のみ\n",
    "        )\n",
    "\n",
    "    # ②\n",
    "    def preprocessing(self, text):\n",
    "        text = re.sub(r'\\n', '', text)\n",
    "        text = re.sub(r'\\r', '', text)\n",
    "        text = re.sub(r'\\d', '', text)\n",
    "        text = re.sub(r'\\s', '', text)\n",
    "        text.replace('\\u3000', '')\n",
    "        text = text.lower()\n",
    "        text = mojimoji.zen_to_han(text, kana=True)\n",
    "        text = mojimoji.han_to_zen(text, digit=False, ascii=False)\n",
    "        text = ''.join(c for c in text if c not in emoji.UNICODE_EMOJI)\n",
    "        text = neologdn.normalize(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    # ③\n",
    "    def make_sentence_list(self, sentences):\n",
    "        doc = self.nlp(sentences)\n",
    "        self.ginza_sents_object = doc.sents\n",
    "        sentence_list = [s for s in doc.sents]\n",
    "\n",
    "        return sentence_list\n",
    "\n",
    "    # ④\n",
    "    def make_corpus(self):\n",
    "        corpus = [' '.join(self.analyzer.analyze(str(s))) + '。' for s in self.ginza_sents_object]\n",
    "\n",
    "        return corpus\n",
    "\n",
    "class EnglishCorpus(JapaneseCorpus):\n",
    "    # ①\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # ②\n",
    "    def preprocessing(self, text):\n",
    "        text = re.sub(r'\\n', '', text)\n",
    "        text = re.sub(r'\\r', '', text)\n",
    "        text = mojimoji.han_to_zen(text, digit=False, ascii=False)\n",
    "        text = mojimoji.zen_to_han(text, kana=True)\n",
    "        text = ''.join(c for c in text if c not in emoji.UNICODE_EMOJI)\n",
    "        text = neologdn.normalize(text)        \n",
    "\n",
    "        return text\n",
    "\n",
    "    # ④\n",
    "    def make_corpus(self):\n",
    "        corpus = []\n",
    "        for s in self.ginza_sents_object:\n",
    "            tokens = [str(t) for t in s]\n",
    "            corpus.append(' '.join(tokens))\n",
    "\n",
    "        return corpus\n",
    "\n",
    "\n",
    "algorithm_dic = {\"lex\": LexRankSummarizer(), \"tex\": TextRankSummarizer(), \"lsa\": LsaSummarizer(),\\\n",
    "                 \"kl\": KLSummarizer(), \"luhn\": LuhnSummarizer(), \"redu\": ReductionSummarizer(),\\\n",
    "                 \"sum\": SumBasicSummarizer()}\n",
    "\n",
    "#文章の内容を要約する関数\n",
    "def summarize_sentences(sentences, sentences_count, algorithm=\"lex\", language=\"japanese\"):\n",
    "    # ①\n",
    "    if language == \"japanese\":\n",
    "        corpus_maker = JapaneseCorpus()\n",
    "    else:\n",
    "        corpus_maker = EnglishCorpus()\n",
    "    preprocessed_sentences = corpus_maker.preprocessing(sentences)\n",
    "    preprocessed_sentence_list = corpus_maker.make_sentence_list(preprocessed_sentences)\n",
    "    corpus = corpus_maker.make_corpus()\n",
    "    parser = PlaintextParser.from_string(\" \".join(corpus), Tokenizer(language))\n",
    "\n",
    "    # ②\n",
    "    try:\n",
    "        summarizer = algorithm_dic[algorithm]\n",
    "    except KeyError:\n",
    "        print(\"algorithm name:'{}'is not found.\".format(algorithm))\n",
    "\n",
    "    summarizer.stop_words = get_stop_words(language)\n",
    "    #sentences_countは文書に一定の割合をかけた値でもよいが、読みやすさ重視で10センテンス固定長とする\n",
    "    #int(len(corpus)*0.1)\n",
    "    summary = summarizer(document=parser.document, sentences_count=10)\n",
    "\n",
    "    # ③\n",
    "    if language == \"japanese\":\n",
    "        return (\"\".join([str(preprocessed_sentence_list[corpus.index(sentence.__str__())]) for sentence in summary]))\n",
    "    else:\n",
    "        return (\" \".join([sentence.__str__() for sentence in summary]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc81db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize quality test\n",
    "\n",
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "\n",
    "#read human summarization csv\n",
    "sum_df_t = pd.read_csv('C:/Users/Administrator/Desktop/Python 3/Graduation Task/human_sum/ss50.csv',encoding='cp932')\n",
    "sum_df_t.人間要約 = sum_df_t.人間要約.str.replace('\\n','')\n",
    "\n",
    "#specify ja\n",
    "nlp = spacy.load('ja_ginza')\n",
    "rouge = RougeCalculator(stopwords=True, lang=\"ja\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#measuring score on each 20 summaries\n",
    "score_dict = {}\n",
    "score_list = []\n",
    "\n",
    "for i in range(20):\n",
    "    print('{}:処理中'.format(i))\n",
    "    for algo in algorithm_dic.keys():\n",
    "        #print(i,algo)\n",
    "        summary = str((summarize_sentences(sentences=sum_df_t.本文[i], sentences_count=10, algorithm=algo, language=\"japanese\")))\n",
    "        rouge_2 = rouge.rouge_n(summary=summary,references=str(sum_df_t.人間要約[i]),n=2)\n",
    "        score_dict[algo] = rouge_2\n",
    "    score_list.append(score_dict)\n",
    "    #dict 初期化\n",
    "    score_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(score_list)\n",
    "result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2c3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ffa4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacyアナライザ\n",
    "nlp = spacy.load('ja_ginza')\n",
    "ginza.set_split_mode(nlp, \"C\") # 形態素の分割モード指定\n",
    "def spacy_analyzer(text):\n",
    "    #正規化\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r'\\u3000','',text)\n",
    "    text.replace('\\u3000', '')\n",
    "    doc = nlp(text)\n",
    "\n",
    "    #結果を格納するリスト\n",
    "    result_list = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.pos_ in ['NOUN', 'ADJ', 'VERB','ADV']:#名詞、形容詞、動詞、副詞のみ\n",
    "                result_list.append(token.text)\n",
    "            \n",
    "        #https://yu-nix.com/blog/2021/3/3/spacy-pos-list/#spaCy%E3%81%AE%E5%93%81%E8%A9%9E%E4%B8%80%E8%A6%A7%EF%BC%88%E6%97%A5%E6%9C%AC%E8%AA%9E%E8%A8%B3%EF%BC%89\n",
    "        #result_list = result_list + [token.text for token in sent if token.pos_ in ['NOUN', 'ADJ', 'VERB','ADV','SCONJ','PART','DET','AUX']]\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mecabアナライザ\n",
    "def mecab_analyzer(text):\n",
    "    #正規化\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    #text = re.sub(r'\\u3000','',text)\n",
    "    text.replace('\\u3000', '')\n",
    "    tagger = MeCab.Tagger(\"-Owakati\")\n",
    "    node = tagger.parseToNode(text)\n",
    "\n",
    "    # 指定した品詞を抽出しリストに\n",
    "    word_list = []\n",
    "    \n",
    "    while node:\n",
    "        word_type = node.feature.split(',')[0]\n",
    "        if word_type in ['名詞', '形容詞', '動詞','副詞','接尾辞']:\n",
    "            word_list.append(node.surface)\n",
    "        #ここのインデントミスると無限ループ\n",
    "        node = node.next\n",
    "    # リストを文字列に変換\n",
    "    word_chain = ' '.join(word_list)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5426c7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6848518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346339e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#全投稿のコサイン類似度を判定したい\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#token格納用リスト\n",
    "token_list = []\n",
    "\n",
    "for review in tqdm(company_df.本文):\n",
    "    review_token = mecab_analyzer(review)\n",
    "    token_list.append(' '.join(review_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e8551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4320bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5336fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfIdfインスタンス作成\n",
    "vec = TfidfVectorizer(analyzer=mecab_analyzer,\n",
    "                      #stop_words=stopword_list,\n",
    "                      min_df=10,#あまりにも出てこないワードは対象外\n",
    "                      ngram_range=(1,2),use_idf=True)#bigram/unigram/を考慮する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7585ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf行列を作成\n",
    "vec.fit(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2703307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf行列作成\n",
    "tfidf_matrix = vec.transform(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad3c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "np.save('./array/tfidf_matrix.pkl', tfidf_matrix, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "np.load('./array/tfidf_matrix.pkl.npy', allow_pickle=True, encoding='ASCII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v model load\n",
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load('./model/w2v_nottrained_skip.model')\n",
    "\n",
    "w2v_words = list(model.wv.vocab)\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "print(\"sample words \", w2v_words[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7aeadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.wv.most_similar('優秀')\n",
    "#model.wv['年功序列']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20736ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vocab and tfidf value dict from tfidf instance\n",
    "dictionary = dict(zip(vec.get_feature_names(), list(vec.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892986e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = vec.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "\n",
    "for review in company_df.本文: # for each review/sentence \n",
    "    sent_vec = np.zeros(200) # as word vectors are of zero length\n",
    "    weight_sum =0 # num of words with a valid vector in the sentence/review\n",
    "    \n",
    "    tagger = MeCab.Tagger(\"-Owakati\")\n",
    "    node = tagger.parseToNode(review)\n",
    "\n",
    "    # 指定した品詞を抽出しリストに\n",
    "    word_list = []\n",
    "    \n",
    "    while node:\n",
    "        word_type = node.feature.split(',')[0]\n",
    "        if word_type in ['名詞', '形容詞', '動詞','副詞','接尾辞']:\n",
    "            word_list.append(node.surface)\n",
    "        #ここのインデントミスると無限ループ\n",
    "        node = node.next\n",
    "    \n",
    "    for word in word_list: # for each word in a review\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = model.wv[word]#200次元のW2Vベクトル\n",
    "            #tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n",
    "            # to reduce the computation we are \n",
    "            # dictionary[word] = idf value of word in whole courpus\n",
    "            # sent.count(word) = tf valeus of word in this review\n",
    "            \n",
    "            #tfidf行列の中の対応する単語のidf値に、レビューごとの\n",
    "            #その単語の登場回数を、レビューの単語数で割ったもの(tf)をかけて、tfidf値としている\n",
    "            tfidf = dictionary[word]*(review.count(word)/len(review))\n",
    "            \n",
    "            #w2v単語ベクトルに、tfidf値をかけることで、tfidfかけたw2v単語ベクトルを獲得\n",
    "            sent_vec += (vec * tfidf)\n",
    "            \n",
    "            #各単語のtfidf値を合計している。\n",
    "            #あるレビューにおける、各単語のtfidf値の合計は、word2vecの各単語にtfidfの重みをかけた\n",
    "            #加重word2vecベクトルに対して、その重みの合計値で割ることで、加重平均ベクトルを\n",
    "            #抽出するために必要なのである。\n",
    "            weight_sum += tfidf\n",
    "            \n",
    "    #かけたものを、文章内の単語のtfidf値合計で割ることで、tfidf加重平均word2vecを出している\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    #ここに重みつけしたベクトルが入る\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1\n",
    "    print(row) #進捗確認用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e08e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ちゃんとtfidfで加重平均とったw2vベクトルが獲得できた\n",
    "#arrayにしておく\n",
    "tfidf_sent_vectors_array = np.array(tfidf_sent_vectors)\n",
    "tfidf_sent_vectors_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存\n",
    "np.save('./array/tfidf_matrix_fixed.pkl', tfidf_sent_vectors_array, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fbe4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意のテキストのベクトルを計算する関数\n",
    "def get_vector(text):\n",
    "    sum_vec = np.zeros(200)\n",
    "    word_count = 0\n",
    "    mecab = MeCab.Tagger('-Owakati')\n",
    "    node = mecab.parseToNode(text)\n",
    "    while node:\n",
    "        fields = node.feature.split(\",\")\n",
    "        # 名詞、動詞、形容詞に限定\n",
    "        if fields[0] == '名詞' or fields[0] == '動詞' or fields[0] == '形容詞' or fields[0] == '副詞' or fields[0] == '接尾辞':\n",
    "            sum_vec += model[node.surface]\n",
    "            word_count += 1\n",
    "        node = node.next\n",
    "    return sum_vec / word_count#全単語ベクトルの平均をとっているだけ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c7c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vector('年功序列')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6d810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e500a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe014a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#完成版\n",
    "def similar_company_f(input_text):\n",
    "    text_similarity = cosine_similarity(get_vector(input_text).reshape(-1,200), tfidf_sent_vectors_array.reshape(-1,200))\n",
    "    # 類似度が0.3以上の要素の数を取り出す\n",
    "    num_of_similarities = (np.sum([x > 0.3 for x in text_similarity]))\n",
    "    #類似度でsortとargsortして、類似度とインデックス双方を取り出す\n",
    "    #topは表示件数\n",
    "    top = 20\n",
    "    top_indices = np.argsort(-text_similarity)[0][:num_of_similarities].tolist()[:top+1]\n",
    "    top_similarity = np.sort(text_similarity.ravel())[::-1][:num_of_similarities].tolist()[:top+1]\n",
    "\n",
    "    #dfのハコを作り\n",
    "    answer_df = pd.DataFrame(columns=['類似度','企業名', '評価', '本文', '職種', '経験年数','現職/退職','新卒/中途','性別'])\n",
    "    #forループで連結する\n",
    "    for a,b in zip(top_indices,top_similarity):\n",
    "        index_data = company_df.iloc[a]\n",
    "        answer_df =answer_df.append({'類似度': b,'企業名': index_data[0], \n",
    "                                     '評価': index_data[4],'本文':index_data[5],\n",
    "                                     '職種':index_data[8],'経験年数':index_data[9],\n",
    "                                     '現職/退職':index_data[10],'新卒/中途':index_data[11],\n",
    "                                     '性別':index_data[12]},ignore_index=True)\n",
    "    \n",
    "    #出来上がったdfを、company_info_dfとマージ\n",
    "    answer_df_marged = pd.merge(answer_df,pd.DataFrame(company_info_df[['企業名','総合スコア']]),on='企業名',how='inner')\n",
    "    \n",
    "    #出来上がったdfを、類以度でsort\n",
    "    answer_df_marged_g = answer_df_marged.set_index(['総合スコア','企業名'])\n",
    "    #最後は総合スコアでSortしたものを返す\n",
    "    #return answer_df_marged_g.sort_index(ascending=False)[:top].sort_index(ascending=False)[:top]\n",
    "    return answer_df_marged_g.sort_values(by='類似度',ascending=False)[:top]\n",
    "\n",
    "similar_company_f('残業が少なく働きやすい')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec1e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語のベクトルを平均を取るのではなく全部保存する関数\n",
    "def get_vector_mod(text):\n",
    "    blank = []\n",
    "    sum_vec = np.zeros(200)\n",
    "    #word_count = 0\n",
    "    mecab = MeCab.Tagger('-Owakati')\n",
    "    node = mecab.parseToNode(text)\n",
    "    while node:\n",
    "        fields = node.feature.split(\",\")\n",
    "        # 名詞、動詞、形容詞に限定\n",
    "        if fields[0] == '名詞' or fields[0] == '動詞' or fields[0] == '形容詞':\n",
    "            blank.append(model[node.surface])\n",
    "        node = node.next\n",
    "        \n",
    "    return np.array(blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b012f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vector_mod('会社で働く').shape #形態素単位でベクトルが保存されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('残業',topn=1000)\n",
    "model.wv[\"残業\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def each_word_similarity(input_word):\n",
    "    \n",
    "    review = \"\"\"平均的に年齢も若く、ワイワイノリの雰囲気があります。\n",
    "    　　　　　　皆頑張って働いていて若手が引っ張っていくという考えが若手全員にありました。\n",
    "          　　　上層部でもそのような考え方には否定的でなくむしろ肯定的で頑張っている若手には\n",
    "             　 チャンスを多く与えてくれました。\"\"\"\n",
    "    \n",
    "    review = \"\"\"若手が活躍している。\"\"\"\n",
    "    \n",
    "    similarity_list = []\n",
    "    \n",
    "    #inputを形態素に分ける\n",
    "\n",
    "    tagger = MeCab.Tagger(\"-Owakati\")\n",
    "    node = tagger.parseToNode(input_word)\n",
    "\n",
    "    # 指定した品詞を抽出しリストに\n",
    "    input_word_list = []\n",
    "    while node:\n",
    "        word_type = node.feature.split(',')[0]\n",
    "        if word_type in ['名詞', '形容詞', '副詞', '動詞']:\n",
    "            input_word_list.append(node.surface)\n",
    "        #ここのインデントミスると無限ループ\n",
    "        node = node.next\n",
    "        \n",
    "    \n",
    "    #レビューを形態素に分ける\n",
    "    tagger2 = MeCab.Tagger(\"-Owakati\")\n",
    "    node2 = tagger.parseToNode(review)\n",
    "\n",
    "    # 指定した品詞を抽出しリストに\n",
    "    input_word_list2 = []\n",
    "    \n",
    "    while node2:\n",
    "        word_type = node2.feature.split(',')[0]\n",
    "        if word_type in ['名詞', '形容詞', '副詞', '動詞']:\n",
    "            input_word_list2.append(node2.surface)\n",
    "        #ここのインデントミスると無限ループ\n",
    "        node2 = node2.next\n",
    "    \n",
    "    #単語単位での類似性判定\n",
    "    for ei in input_word_list:\n",
    "        for er in input_word_list2:\n",
    "            if er in [al[0] for al in model.wv.most_similar(ei,topn=100)] + [ei]:\n",
    "            try:\n",
    "                similarity = cosine_similarity(model.wv[ei].reshape(-1,200), model.wv[er].reshape(-1,200)).ravel()[0]\n",
    "                #similarity = cosine_similarity(get_vector_mod(ei).reshape(-1,200), model.wv[er].reshape(-1,200)).ravel()[0]\n",
    "                similarity_list.append(similarity)\n",
    "            except ValueError:\n",
    "                pass\n",
    "                \n",
    "    print(similarity_list)\n",
    "    print(input_word_list)\n",
    "    print(input_word_list2)\n",
    "    #単語ベースでの平均類以度を算出\n",
    "    return np.mean(similarity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc024f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"若手が活躍している\"\"\"\n",
    "each_word_similarity(review) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae26bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定した品詞を抽出しリストに\n",
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "node = tagger.parseToNode('若手が活躍している')\n",
    "input_word_list = []\n",
    "while node:\n",
    "    word_type = node.feature.split(',')[0]\n",
    "    if word_type in ['名詞', '形容詞', '副詞', '動詞']:\n",
    "        input_word_list.append(node.surface)\n",
    "        #ここのインデントミスると無限ループ\n",
    "    node = node.next\n",
    "    \n",
    "input_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = vec.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "\n",
    "for review in company_df.本文: # for each review/sentence \n",
    "    sent_vec = np.zeros(200) # as word vectors are of zero length\n",
    "    weight_sum =0 # num of words with a valid vector in the sentence/review\n",
    "    \n",
    "    tagger = MeCab.Tagger(\"-Owakati\")\n",
    "    node = tagger.parseToNode(review)\n",
    "\n",
    "    # 指定した品詞を抽出しリストに\n",
    "    word_list = []\n",
    "    \n",
    "    while node:\n",
    "        word_type = node.feature.split(',')[0]\n",
    "        if word_type in ['名詞', '形容詞', '副詞', '動詞']:\n",
    "            word_list.append(node.surface)\n",
    "        #ここのインデントミスると無限ループ\n",
    "        node = node.next\n",
    "    \n",
    "    for word in word_list: # for each word in a review\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = model.wv[word]#200次元のW2Vベクトル\n",
    "            #tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n",
    "            # to reduce the computation we are \n",
    "            # dictionary[word] = idf value of word in whole courpus\n",
    "            # sent.count(word) = tf valeus of word in this review\n",
    "            \n",
    "            #tfidf行列の中の対応する単語のidf値に、レビューごとの\n",
    "            #その単語の登場回数を、レビューの単語数で割ったもの(tf)をかけて、tfidf値としている\n",
    "            tfidf = dictionary[word]*(review.count(word)/len(review))\n",
    "            \n",
    "            #w2v単語ベクトルに、tfidf値をかけることで、tfidfかけたw2v単語ベクトルを獲得\n",
    "            sent_vec += (vec * tfidf)\n",
    "            \n",
    "            #各単語のtfidf値を合計している。\n",
    "            #あるレビューにおける、各単語のtfidf値の合計は、word2vecの各単語にtfidfの重みをかけた\n",
    "            #加重word2vecベクトルに対して、その重みの合計値で割ることで、加重平均ベクトルを\n",
    "            #抽出するために必要なのである。\n",
    "            weight_sum += tfidf\n",
    "            \n",
    "    #かけたものを、文章内の単語のtfidf値合計で割ることで、tfidf加重平均word2vecを出している\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    #ここに重みつけしたベクトルが入る\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1\n",
    "    print(row) #進捗確認用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2588bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ベクトルの平均を取るのではなく、inputtextに類似する単語の類以度を平均する方法\n",
    "def similar_company_f(input_text):\n",
    "    text_similarity = cosine_similarity(get_vector(input_text).reshape(-1,200), tfidf_sent_vectors_array.reshape(-1,200))\n",
    "    # 類似度が0.3以上の要素の数を取り出す\n",
    "    num_of_similarities = (np.sum([x > 0.3 for x in text_similarity]))\n",
    "    #類似度でsortとargsortして、類似度とインデックス双方を取り出す\n",
    "    #topは表示件数\n",
    "    top = 20\n",
    "    top_indices = np.argsort(-text_similarity)[0][:num_of_similarities].tolist()[:top+1]\n",
    "    top_similarity = np.sort(text_similarity.ravel())[::-1][:num_of_similarities].tolist()[:top+1]\n",
    "\n",
    "    #dfのハコを作り\n",
    "    answer_df = pd.DataFrame(columns=['類似度','企業名', '評価', '本文', '職種', '経験年数','現職/退職','新卒/中途','性別'])\n",
    "    #forループで連結する\n",
    "    for a,b in zip(top_indices,top_similarity):\n",
    "        index_data = company_df.iloc[a]\n",
    "        answer_df =answer_df.append({'類似度': b,'企業名': index_data[0], \n",
    "                                     '評価': index_data[4],'本文':index_data[5],\n",
    "                                     '職種':index_data[8],'経験年数':index_data[9],\n",
    "                                     '現職/退職':index_data[10],'新卒/中途':index_data[11],\n",
    "                                     '性別':index_data[12]},ignore_index=True)\n",
    "    \n",
    "    #出来上がったdfを、company_info_dfとマージ\n",
    "    answer_df_marged = pd.merge(answer_df,pd.DataFrame(company_info_df[['企業名','総合スコア']]),on='企業名',how='inner')\n",
    "    \n",
    "    #出来上がったdfを、類以度でsort\n",
    "    answer_df_marged_g = answer_df_marged.set_index(['総合スコア','企業名'])\n",
    "    #最後は総合スコアでSortしたものを返す\n",
    "    #return answer_df_marged_g.sort_index(ascending=False)[:top].sort_index(ascending=False)[:top]\n",
    "    return answer_df_marged_g.sort_values(by='類似度',ascending=False)[:top]\n",
    "\n",
    "similar_company_f('雰囲気が良い')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4142058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tfidf_matrix = pickle.load(open(\"tfidf_matrix.pkl\", \"rb\"))\n",
    "\n",
    "vec = pickle.load(open(\"tfidf_vec.pkl\", \"rb\"))\n",
    "vec.set_params(analyzer=mecab_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "vec.set_params(analyzer='word')\n",
    "# 保存 \n",
    "pickle.dump(vec, open(\"tfidf_vec.pkl\", \"wb\"))\n",
    "pickle.dump(tfidf_matrix,open('tfidf_matrix.pkl',\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8bab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtype_dict = {}\n",
    "with open('./jobtype.csv','r',encoding='cp932') as f:\n",
    "    for k in f:\n",
    "        ab = k.strip().split(',')\n",
    "        if ab[0] not in jobtype_dict:\n",
    "            jobtype_dict[ab[0]] = ab[1]\n",
    "\n",
    "            \n",
    "\n",
    "#職種の分類は以下の通り        \n",
    "\"\"\"\n",
    "その他\n",
    "コーポレート・管理・総務\n",
    "営業職\n",
    "事務\n",
    "マーケティング職\n",
    "製造\n",
    "研究開発\n",
    "人事\n",
    "開発・設計\n",
    "企画系\n",
    "経理財務\n",
    "SE/PM系\n",
    "マネジメント\n",
    "エンジニア系\n",
    "デザイナー\n",
    "法務\n",
    "\n",
    "\"\"\"\n",
    "#以下にjobtypeの辞書を用意した\n",
    "jobtype_list_dict = {0:'その他',1:\"コーポレート・管理・総務\",2:'営業職・販売',3:'事務',4:'マーケティング職',\n",
    "                5:'製造',6:'研究開発',7:'人事',8:'開発・設計・技術',9:'生産管理・調達・物流',10:'マネジメント',11:'企画系',\n",
    "                12:'現業',13:'経理財務',14:'SE/PM系',15:'エンジニア系',16:'専門職',17:'コンサルタント',18:'デザイナー',19:'法務'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c5e84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ff3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#任意の文字列の入力と、職種IDをうけとり、指定した職種IDにマッチし、入力と類似したレビューを持つ企業名とレビューを返す関数\n",
    "def similar_company(input_text,jobtype=None):\n",
    "    '''\n",
    "    見たいレビューの職種を限定したい場合は\n",
    "    以下のjobtype_listのindex番号を引数で渡し、指定すること(指定しなくてもOK)\n",
    "    (基本的には「その他」は分類不能なもの(「総合職」、「本社」、「～～会社xx事業部」)が多いです)\n",
    "    \n",
    "jobtype_list_dict = {0:'その他',1:\"コーポレート・管理・総務\",2:'営業職・販売',3:'事務',4:'マーケティング職',\n",
    "                5:'製造',6:'研究開発',7:'人事',8:'開発・設計・技術',9:'生産管理・調達・物流',10:'マネジメント',11:'企画系',\n",
    "                12:'現業',13:'経理財務',14:'SE/PM系',15:'エンジニア系',16:'専門職',17:'コンサルタント',18:'デザイナー',19:'法務'}\n",
    "    \n",
    "    '''\n",
    "    #tokenization\n",
    "    tokens = mecab_analyzer(input_text)\n",
    "    #トークンを格納したリストを作成\n",
    "    token_list = []\n",
    "    token_list.append(' '.join(tokens))\n",
    "    sample_vector = vec.transform(token_list)\n",
    "\n",
    "    # 計算した文書のtfidf_matrixと指定した文字列のベクトルのコサイン類似度を計算\n",
    "    text_similarity = cosine_similarity(sample_vector, tfidf_matrix)\n",
    "    \n",
    "    # 類似度が0.4以上の要素の数を取り出す\n",
    "    num_of_similarities = (np.sum([x > 0.3 for x in text_similarity]))\n",
    "\n",
    "    #類似度でsortとargsortして、類似度とインデックス双方を取り出す\n",
    "    top_indices = np.argsort(-text_similarity)[0][:num_of_similarities].tolist()\n",
    "    top_similarity = np.sort(text_similarity.ravel())[::-1][:num_of_similarities].tolist()\n",
    "    \n",
    "    #dfのハコを作り\n",
    "    answer_df = pd.DataFrame(columns=['類似度','企業名', '評価', '本文', '職種', '経験年数','現職/退職','新卒/中途','性別'])\n",
    "    \n",
    "    #カラム内の文字数。デフォルトは50なので変更\n",
    "    pd.set_option(\"display.max_colwidth\", 1500)\n",
    "    \n",
    "    #行数上限も変更し\n",
    "    pd.set_option(\"display.max_rows\", 101)\n",
    "\n",
    "    #forループで連結する\n",
    "    for a,b in zip(top_indices,top_similarity):\n",
    "        index_data = company_df.iloc[a]\n",
    "        answer_df =answer_df.append({'類似度': b,'企業名': index_data[0], \n",
    "                                     '評価': index_data[4],'本文':index_data[5],\n",
    "                                     '職種':index_data[8],'経験年数':index_data[9],\n",
    "                                     '現職/退職':index_data[10],'新卒/中途':index_data[11],\n",
    "                                     '性別':index_data[12]},ignore_index=True)\n",
    "        \n",
    "    #出来上がったdfを、company_info_dfとマージ\n",
    "    answer_df_marged = pd.merge(answer_df,pd.DataFrame(company_info_df[['企業名','総合スコア']]),on='企業名',how='inner')\n",
    "    \n",
    "    #企業風土の評価スコアはdrop\n",
    "    #answer_df_marged = answer_df_marged.drop('評価',axis=1)\n",
    "    #出来上がったdfを、類以度でsort　topは表示件数\n",
    "    top = 30\n",
    "    answer_df_marged_g = answer_df_marged.set_index(['総合スコア','企業名'])\n",
    "    \n",
    "    #jobtypeの入力に応じて出力を変える\n",
    "    #会社の総合スコア順に表示\n",
    "    \n",
    "    if jobtype is None:\n",
    "        return answer_df_marged_g.sort_index(ascending=False)[:top]\n",
    "    \n",
    "    else:\n",
    "        answer_df_marged_g['職種'] =  answer_df_marged_g['職種'].map(jobtype_dict)\n",
    "        job_adm = answer_df_marged_g[answer_df_marged_g['職種'] == jobtype_list_dict[jobtype]].reset_index()\n",
    "        job_adm_s = job_adm.set_index(['総合スコア','企業名'])\n",
    "        return job_adm_s.sort_index(ascending=False)[:top]\n",
    "        \n",
    "        \n",
    "        \n",
    "    #return answer_df_marged_g[:top].sort_values('総合スコア',ascending=False)\n",
    "\n",
    "    #return answer_df_marged.sort_values(['総合スコア'],ascending=False).reset_index().drop('index',axis=1)[:top]\n",
    "\n",
    "\n",
    "#ここに検索したい企業文化をフリーワードで入力する\n",
    "input_text = '残業が少なくて働きやすい'\n",
    "similar_company(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b70e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04863e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのロード(モデルが用意してあれば、ここからで良い)\n",
    "m = Doc2Vec.load('./model/openwork.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5245f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#企業ごとコメント全部をベクトル化\n",
    "#まず企業毎のレビューを全部くっつけて、1企業全レビューのdfにする\n",
    "all_docs = pd.DataFrame(company_df.groupby('企業名').本文.apply(list).apply(' '.join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab2698",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#token格納用リスト\n",
    "token_list = []\n",
    "    \n",
    "for review in all_docs.本文:\n",
    "    review_token = mecab_analyzer(review)\n",
    "    token_list.append(' '.join(review_token))\n",
    "    \n",
    "trainings = [TaggedDocument(words = data.split(' '),tags = [i]) for i,data in enumerate(token_list)]\n",
    "\n",
    "#Doc2Vecをためす\n",
    "#parameter: https://github.com/jkatsuta/18_4q_doc2vec/blob/master/experiments/exp_params_setup1.ipynb\n",
    "model = Doc2Vec(documents=trainings, dm=1,\n",
    "                min_alpha=1e-4, size=200,\n",
    "                window=10, min_count=10,\n",
    "                sample=1e-3,\n",
    "                worker=-1,epochs=150#epoch=150\n",
    "                )\n",
    "\n",
    "\n",
    "print('\\n訓練開始')\n",
    "for epoch in range(150):\n",
    "    print('Epoch: {}'.format(epoch + 1))\n",
    "    model.train(trainings,total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "\n",
    "# モデルのセーブ\n",
    "model.save(\"./model/openwork_uniquecompany_dbow.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4989d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "compname = 'ヤフー株式会社'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f17ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのロード(モデルが用意してあれば、ここからで良い)\n",
    "#m = Doc2Vec.load(\"./model/openwork_uniquecompany_dbow.model\")\n",
    "m = Doc2Vec.load(\"./model/old/openwork_uniquecompany.model\")\n",
    "\n",
    "#企業名を引数にとり、その企業のコメントベクトルに類似したベクトルを持つ企業名と類似度を表示する関数\n",
    "def company_comparison(company_name):\n",
    "    identified_company_docs = all_docs[all_docs.index == company_name]\n",
    "    input_text = identified_company_docs.本文.str.cat()\n",
    "    #Mecabによって形態素解析\n",
    "    mecab_input = mecab_analyzer(input_text)\n",
    "    \n",
    "    #docvecsによる類似性判定結果\n",
    "    #類似度TOP10の企業を表示\n",
    "    similarity = (m.docvecs.most_similar(positive=[m.infer_vector(mecab_input)],topn=10))\n",
    "    \n",
    "    #格納用空リスト\n",
    "    return_list=[]\n",
    "    for num,similarity in similarity:\n",
    "        return_list.append([all_docs.index[num],similarity])\n",
    "    #df化\n",
    "    output_df = pd.DataFrame(return_list,columns=['企業名','類似度']).sort_values('類似度',ascending=False)\n",
    "    return output_df[1:] #自社が最も類似しているので、1行目は除いて、2行目から表示\n",
    "    \n",
    "    \n",
    "company_comparison(compname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed74d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc1c90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = Doc2Vec.load(\"./model/openwork_uniquecompany_dbow.model\")\n",
    "#m = Doc2Vec.load(\"./model/old/openwork_uniquecompany.model\")\n",
    "\n",
    "#企業名を引数にとり、その企業のコメントベクトルに類似したベクトルを持つ企業名と類似度を表示する関数\n",
    "def company_comparison(company_name):\n",
    "    identified_company_docs = all_docs[all_docs.index == company_name]\n",
    "    input_text = identified_company_docs.本文.str.cat()\n",
    "    #Mecabによって形態素解析\n",
    "    mecab_input = mecab_analyzer(input_text)\n",
    "    \n",
    "    #docvecsによる類似性判定結果\n",
    "    #類似度TOP10の企業を表示\n",
    "    similarity = (m.docvecs.most_similar(positive=[m.infer_vector(mecab_input)],topn=10))\n",
    "    \n",
    "    #格納用空リスト\n",
    "    return_list=[]\n",
    "    for num,similarity in similarity:\n",
    "        return_list.append([all_docs.index[num],similarity])\n",
    "    #df化\n",
    "    output_df = pd.DataFrame(return_list,columns=['企業名','類似度']).sort_values('類似度',ascending=False)\n",
    "    return output_df[1:] #自社が最も類似しているので、1行目は除いて、2行目から表示\n",
    "    \n",
    "    \n",
    "company_comparison(compname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba4abfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa18f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de934ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6aea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#検索したいワード\n",
    "input_text = '残業が少なく働きやすい'\n",
    "\n",
    "#Mecabによって形態素解析\n",
    "mecab_input = mecab_analyzer(input_text)\n",
    "\n",
    "#docvecsプロパティのmost_similarメソッドを使用すると、\n",
    "#ラベル付した記事からコサイン類似度を計算して類似した記事を取得することができる\n",
    "#類似文書のindexを取得\n",
    "#positive:類似 negative:類似していない\n",
    "print(m.docvecs.most_similar(positive=[m.infer_vector(mecab_input)],topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a125c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb455f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8abd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#企業毎にコメント数がバラバラな問題があるので、抽出件数を揃えたい\n",
    "#企業ごとコメントを8件RANDOMに抽出し、ベクトル化したらどうなるか\n",
    "AK = pd.DataFrame(company_df.groupby('企業名').本文.apply(lambda x:x.sample(n=5,random_state=0,replace=True)))\n",
    "#df.sampleにかんして参照 https://analytics-note.xyz/programming/dataframe-sample-doc/\n",
    "\n",
    "all_docs_5sentences = pd.DataFrame(AK.groupby('企業名').本文.apply(list).apply(''.join))\n",
    "\n",
    "#token格納用リスト\n",
    "token_list = []\n",
    "    \n",
    "for review in all_docs_5sentences.本文:\n",
    "    review_token = mecab_analyzer(review)\n",
    "    token_list.append(' '.join(review_token))\n",
    "    \n",
    "trainings = [TaggedDocument(words = data.split(' '),tags = [i]) for i,data in enumerate(token_list)]\n",
    "\n",
    "#parameter: https://github.com/jkatsuta/18_4q_doc2vec/blob/master/experiments/exp_params_setup1.ipynb\n",
    "model = Doc2Vec(documents=trainings, dm=1,\n",
    "                min_alpha=.1e-4, size=200,\n",
    "                window=10, min_count=5,\n",
    "                worker=-1,epochs=50\n",
    "                )\n",
    "\n",
    "\n",
    "print('\\n訓練開始')\n",
    "for epoch in range(50):\n",
    "    print('Epoch: {}'.format(epoch + 1))\n",
    "    model.train(trainings,total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "\n",
    "# モデルのセーブ\n",
    "model.save(\"./model/openwork_uniquecompany_8sentences.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b049496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのロード(モデルが用意してあれば、ここからで良い)\n",
    "m = Doc2Vec.load(\"./model/openwork_uniquecompany_8sentences.model\")\n",
    "\n",
    "#企業名を引数にとり、その企業のコメントベクトルに類似したベクトルを持つ企業名と類似度を表示する関数\n",
    "def company_comparison(company_name):\n",
    "    identified_company_docs = all_docs[all_docs.index == company_name]\n",
    "    input_text = identified_company_docs.本文.str.cat()\n",
    "    #Mecabによって形態素解析\n",
    "    mecab_input = mecab_analyzer(input_text)\n",
    "    \n",
    "    #docvecsによる類似性判定結果\n",
    "    #類似度TOP10の企業を表示\n",
    "    similarity = (m.docvecs.most_similar(positive=[m.infer_vector(mecab_input)],topn=10))\n",
    "    \n",
    "    #格納用空リスト\n",
    "    return_list=[]\n",
    "    for num,similarity in similarity:\n",
    "        return_list.append([all_docs.index[num],similarity])\n",
    "    #df化\n",
    "    output_df = pd.DataFrame(return_list,columns=['企業名','類似度']).sort_values('類似度',ascending=False)\n",
    "    return output_df[1:] #自社が最も類似しているので、1行目は除いて、2行目から表示\n",
    "    \n",
    "company_comparison(compname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値カラムを用意\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "scaling_columns = ['月残業時間', '有給消化率', '平均年収', '社員の士気', '風通し', '社員相互尊重',\n",
    "       '20代成長性', '長期育成', 'コンプラ',  '待遇満足度', '会社年齢'] \n",
    "\n",
    "#StandardScale 使うのはこっちのデータ\n",
    "SS = StandardScaler()\n",
    "SS.fit(company_info_df_processed[scaling_columns])\n",
    "\n",
    "Sdata = pd.DataFrame(SS.transform(company_info_df_processed[scaling_columns]),columns=scaling_columns)\n",
    "\n",
    "Sdata_m = pd.concat([Sdata,pd.DataFrame(company_info_df_processed.企業名).reset_index()], axis=1,\n",
    "                     ignore_index=True)\n",
    "\n",
    "Sdata_m = Sdata_m.drop(11,axis=1)\n",
    "\n",
    "Sdata_m.columns=['月残業時間', '有給消化率', '平均年収', '社員の士気', '風通し', '社員相互尊重',\n",
    "       '20代成長性', '長期育成', 'コンプラ',  '待遇満足度', '会社年齢','企業名']\n",
    "\n",
    "Sdata_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba9cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1bd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vecの類以度(top1類以度)の、company_info_dfベクトルでみたときの類似度との二乗誤差を計算\n",
    "def cos_sim_error(company_name):\n",
    "    m = Doc2Vec.load(\"./model/openwork_uniquecompany_dbow.model\")\n",
    "    #m = Doc2Vec.load(\"./model/old/openwork_uniquecompany.model\")\n",
    "    identified_company_docs = all_docs[all_docs.index == company_name]\n",
    "    input_text = identified_company_docs.本文.str.cat()\n",
    "    #Mecabによって形態素解析\n",
    "    mecab_input = mecab_analyzer(input_text)\n",
    "    \n",
    "    #doc2vecによる類似性判定結果\n",
    "    #類似度TOP2の企業を表示\n",
    "    similarity = (m.docvecs.most_similar(positive=[m.infer_vector(mecab_input)],topn=2))\n",
    "    \n",
    "    #自社は一番目なので2番めのindex(compname)を取り出す\n",
    "    most_similar_compname =  all_docs.index[similarity[1][0]]\n",
    "    #その類以度も取り出す\n",
    "    doc2vec_similarity =  similarity[1][1]\n",
    "    \n",
    "    #Doc2Vecが指示した、最も類似した企業名を使って、company_info_dfのベクトルでの類似度を求める\n",
    "    #例外は用意しておく(doc2vecが提示した類似企業がcompany_info_dfにない場合があるので)\n",
    "    try:\n",
    "        cinfo_sim = cosine_similarity(Sdata_m[Sdata_m.企業名 == company_name].drop('企業名',axis=1), Sdata_m[Sdata_m.企業名 == most_similar_compname].drop('企業名',axis=1))\n",
    "        #それぞれの類以度の二乗誤差を返す\n",
    "        se_cinfo_sim = ((cinfo_sim - doc2vec_similarity)**2).ravel()[0]\n",
    "        \n",
    "    except ValueError:\n",
    "        #doc2vecが提示した類似企業がcompany_info_dfにない場合,二乗誤差はNANとする\n",
    "        se_cinfo_sim = np.nan\n",
    "    \n",
    "    finally:\n",
    "        return se_cinfo_sim\n",
    "\n",
    "#レビューを持つ企業全てに対して、doc2vecでの類以度と、company_info_dfベクトルの類似度で、top1類以度の企業の類以度二乗誤差を計算\n",
    "se = Sdata_m.企業名.apply(cos_sim_error)\n",
    "se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966c86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2vecの類以度(top1類以度)の、company_info_dfベクトルでみたときの類似度との二乗誤差を計算\n",
    "def cos_sim_error_50(company_name):\n",
    "    #m = Doc2Vec.load(\"./model/openwork_uniquecompany_dbow.model\")\n",
    "    m = Doc2Vec.load(\"./model/old/openwork_uniquecompany.model\")\n",
    "    identified_company_docs = all_docs[all_docs.index == company_name]\n",
    "    input_text = identified_company_docs.本文.str.cat()\n",
    "    #Mecabによって形態素解析\n",
    "    mecab_input = mecab_analyzer(input_text)\n",
    "    \n",
    "    #doc2vecによる類似性判定結果\n",
    "    #類似度TOP2の企業を表示\n",
    "    similarity = (m.docvecs.most_similar(positive=[m.infer_vector(mecab_input)],topn=2))\n",
    "    \n",
    "    #自社は一番目なので2番めのindex(compname)を取り出す\n",
    "    most_similar_compname =  all_docs.index[similarity[1][0]]\n",
    "    #その類以度も取り出す\n",
    "    doc2vec_similarity =  similarity[1][1]\n",
    "    \n",
    "    #Doc2Vecが指示した、最も類似した企業名を使って、company_info_dfのベクトルでの類似度を求める\n",
    "    #例外は用意しておく(doc2vecが提示した類似企業がcompany_info_dfにない場合があるので)\n",
    "    try:\n",
    "        cinfo_sim = cosine_similarity(Sdata_m[Sdata_m.企業名 == company_name].drop('企業名',axis=1), Sdata_m[Sdata_m.企業名 == most_similar_compname].drop('企業名',axis=1))\n",
    "        #それぞれの類以度の二乗誤差を返す\n",
    "        se_cinfo_sim = ((cinfo_sim - doc2vec_similarity)**2).ravel()[0]\n",
    "        \n",
    "    except ValueError:\n",
    "        #doc2vecが提示した類似企業がcompany_info_dfにない場合,二乗誤差はNANとする\n",
    "        se_cinfo_sim = np.nan\n",
    "    \n",
    "    finally:\n",
    "        return se_cinfo_sim\n",
    "\n",
    "#レビューを持つ企業全てに対して、doc2vecでの類以度と、company_info_dfベクトルの類似度で、top1類以度の企業の類以度二乗誤差を計算\n",
    "se50 = Sdata_m.企業名.apply(cos_sim_error_50)\n",
    "se50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5e26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7925445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f37c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BM25を試す\n",
    "import spacy\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load('ja_ginza')\n",
    "text_list = token_list\n",
    "tok_text=[] # for our tokenised corpus\n",
    "\n",
    "\"\"\"\n",
    "token.text, # テキスト\n",
    "token.lemma_, # レンマ\n",
    "token.pos_, # 品詞\n",
    "token.tag_, # 品詞詳細\n",
    "token.dep_, # 構文従属関係\n",
    "token.shape_, # 正書法の特徴(x:文字,d:数値)\n",
    "token.is_alpha, # 文字かどうか\n",
    "token.is_stop # ストップリストの一部かどうか\n",
    "\"\"\"\n",
    "#refrence: https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "#Tokenising using SpaCy:\n",
    "#for doc in tqdm(nlp.pipe(text_list, disable=[\"tagger\", \"parser\",\"ner\"])):\n",
    "#    tok = [t.text for t in doc if t.is_alpha]\n",
    "#    tok_text.append(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d99ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./bygram_list/token_text.pkl', 'wb') as f:\n",
    "#    pickle.dump(tok_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc65d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Plus as BM25\n",
    "bm25 = BM25(tok_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"優秀な人が多い。スピード感がある。\"\n",
    "tokenized_query = mecab_analyzer(query)\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "#top n\n",
    "n=10\n",
    "\n",
    "results = bm25.get_top_n(tokenized_query, company_df.本文.values, n=n)\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "topnscores = np.sort(scores)[::-1][:n].tolist()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'Searched 39,285 records in {round(t1-t0,3) } seconds \\n')\n",
    "for i,k in zip(topnscores,results):\n",
    "    print('{:.2f}'.format(i)+' '+k+'\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf3537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b2185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = bm25.get_scores(tokenized_query)\n",
    "np.min(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29955634",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"優秀な人が多い。スピード感がある。\"\n",
    "tokenized_query = mecab_analyzer(query)\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "np.min(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8900ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v array load\n",
    "allsp_array = np.load('./array/w2v.pkl.npy',allow_pickle=True)\n",
    "allsp_array_fixed = np.array(pd.DataFrame(allsp_array).fillna(0))\n",
    "\n",
    "#query to vector\n",
    "text_vector = get_vector(query)\n",
    "\n",
    "#文書間のcos類似度を計算 \n",
    "text_similarity = cosine_similarity(text_vector.reshape(-1,200), allsp_array_fixed).flatten()\n",
    "\n",
    "#BM25のスコアと,W2Vでの類以度同士の掛け算を行う\n",
    "BM25W2V = np.array(text_similarity * np.array(scores))\n",
    "\n",
    "print(BM25W2V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80acfb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#類似度でsortとargsortして、類似度とインデックス双方を取り出す topn件\n",
    "top=20\n",
    "top_indices = np.argsort(-BM25W2V).tolist()[:top+1]\n",
    "top_similarity = np.sort(BM25W2V)[::-1].tolist()[:top+1]\n",
    "\n",
    "#dfのハコを作り\n",
    "answer_df = pd.DataFrame(columns=['類似度','企業名', '評価', '本文', '職種', '経験年数','現職/退職','新卒/中途','性別'])\n",
    "    \n",
    "#カラム内の文字数。デフォルトは50なので変更\n",
    "pd.set_option(\"display.max_colwidth\", 1500)\n",
    "    \n",
    "#行数上限も変更し\n",
    "pd.set_option(\"display.max_rows\", 101)\n",
    "\n",
    "#forループで連結する\n",
    "for a,b in zip(top_indices,top_similarity):\n",
    "    index_data = company_df.iloc[a]\n",
    "    answer_df =answer_df.append({'類似度': b,'企業名': index_data[0], \n",
    "                                     '評価': index_data[4],'本文':index_data[5],\n",
    "                                     '職種':index_data[8],'経験年数':index_data[9],\n",
    "                                     '現職/退職':index_data[10],'新卒/中途':index_data[11],\n",
    "                                     '性別':index_data[12]},ignore_index=True)\n",
    "        \n",
    "    \n",
    "#出来上がったdfを、company_info_dfとマージ\n",
    "answer_df_marged = pd.merge(answer_df,pd.DataFrame(company_info_df[['企業名','総合スコア']]),on='企業名',how='inner')\n",
    "    \n",
    "#企業風土の評価スコアはdrop\n",
    "#answer_df_marged = answer_df_marged.drop('評価',axis=1)\n",
    "#出来上がったdfを、類以度でsort　topは表示件数\n",
    "\n",
    "answer_df_marged_g = answer_df_marged.set_index(['総合スコア','企業名'])\n",
    "\n",
    "answer_df_marged[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83be0fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bygramの辞書を作成するパート\n",
    "#2gram類以度判定\n",
    "bygram_list = []\n",
    "\n",
    "#n_gram抽出関数\n",
    "def n_gram(target, n):\n",
    "  # 基準を1文字(単語)ずつ ずらしながらn文字分抜き出す\n",
    "  return [target[idx:idx+n] for idx in range(len(target) - n + 1)]\n",
    "\n",
    "for k in tqdm(token_list):\n",
    "    words = k.split(' ')\n",
    "    bygram_list.append(n_gram(words,2))\n",
    "    \n",
    "bygram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '優秀な人が多い。スピード感がある。'\n",
    "\n",
    "\"\"\"\n",
    "#bygramのリストを作成するパート\n",
    "#2gram類以度判定\n",
    "bygram_list = []\n",
    "\n",
    "#n_gram抽出関数\n",
    "def n_gram(target, n):\n",
    "  # 基準を1文字(単語)ずつ ずらしながらn文字分抜き出す\n",
    "  return [target[idx:idx+n] for idx in range(len(target) - n + 1)]\n",
    "\n",
    "for k in (tqdm(token_list)):\n",
    "    words = k.split(' ')\n",
    "    bygram_list.append(n_gram(words,2))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./bygram_list/bygram.pkl', 'wb') as f:\n",
    "#    pickle.dump(bygram_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./bygram_list/bygram.pkl','rb') as f:\n",
    "    bygram_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ec2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bygram_score = []\n",
    "tokenized_query = mecab_analyzer(query)\n",
    "\n",
    "for p in tqdm(bygram_list):\n",
    "    bygram_inclusion = 0\n",
    "    \n",
    "    for k in n_gram(tokenized_query,2):\n",
    "        bygram_inclusion += p.count(k)\n",
    "        \n",
    "    #print(bygram_inclusion)\n",
    "    \n",
    "    try:\n",
    "        #divider = len(p)+1/2\n",
    "        bygram_score.append(bygram_inclusion/(len(p)-1))        \n",
    "    except ZeroDivisionError:\n",
    "        bygram_score.append(0)\n",
    "        \n",
    "print(bygram_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f77e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(bygram_score)\n",
    "company_df.本文.iloc[9604]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c5bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7a9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eda2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v array load\n",
    "allsp_array = np.load('./array/w2v.pkl.npy',allow_pickle=True)\n",
    "allsp_array_fixed = np.array(pd.DataFrame(allsp_array).fillna(0))\n",
    "\n",
    "#query to vector\n",
    "text_vector = get_vector(query)\n",
    "#文書間のcos類似度を計算 \n",
    "text_similarity = cosine_similarity(text_vector.reshape(-1,200), allsp_array_fixed).flatten()\n",
    "\n",
    "#W2Vでの類以度+　BM25スコア*Bygram単位での類以度\n",
    "#bygramw2v = np.array(text_similarity + (np.array(scores) * np.array(bygram_score)))\n",
    "bygramw2v = np.array(bygram_score)\n",
    "\n",
    "#類似度でsortとargsortして、類似度とインデックス双方を取り出す topn件\n",
    "top=20\n",
    "top_indices = np.argsort(-bygramw2v).tolist()[:top+1]\n",
    "top_similarity = np.sort(bygramw2v)[::-1].tolist()[:top+1]\n",
    "\n",
    "#dfのハコを作り\n",
    "answer_df = pd.DataFrame(columns=['類似度','企業名', '評価', '本文', '職種', '経験年数','現職/退職','新卒/中途','性別'])\n",
    "    \n",
    "#カラム内の文字数。デフォルトは50なので変更\n",
    "pd.set_option(\"display.max_colwidth\", 1500)\n",
    "    \n",
    "#行数上限も変更し\n",
    "pd.set_option(\"display.max_rows\", 101)\n",
    "\n",
    "#forループで連結する\n",
    "for a,b in zip(top_indices,top_similarity):\n",
    "    index_data = company_df.iloc[a]\n",
    "    answer_df =answer_df.append({'類似度': b,'企業名': index_data[0], \n",
    "                                     '評価': index_data[4],'本文':index_data[5],\n",
    "                                     '職種':index_data[8],'経験年数':index_data[9],\n",
    "                                     '現職/退職':index_data[10],'新卒/中途':index_data[11],\n",
    "                                     '性別':index_data[12]},ignore_index=True)\n",
    "\n",
    "\n",
    "#出来上がったdfを、company_info_dfとマージ\n",
    "answer_df_marged = pd.merge(answer_df,company_info_df[['企業名','総合スコア']],on='企業名',how='left')\n",
    "\n",
    "#企業風土の評価スコアはdrop\n",
    "#answer_df_marged = answer_df_marged.drop('評価',axis=1)\n",
    "#出来上がったdfを、類以度でsort　topは表示件数\n",
    "\n",
    "answer_df_marged_g = answer_df_marged.set_index(['総合スコア','企業名'])\n",
    "answer_df_marged.sort_values(by='類似度',ascending=False)[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2219b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#最終完成形検索アルゴリズム\n",
    "from rank_bm25 import BM25Plus\n",
    "import spacy\n",
    "import gensim\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#トークンリスト読み込み\n",
    "with open('./bygram_list/token_text.pkl','rb') as f:\n",
    "    tok_text = pickle.load(f)\n",
    "\n",
    "#w2vで作成した単語array読み込み\n",
    "allsp_array = np.load('./array/w2v.pkl.npy',allow_pickle=True)\n",
    "allsp_array_fixed = np.array(pd.DataFrame(allsp_array).fillna(0))\n",
    "\n",
    "#レビュー本文のbygramリスト読み込み\n",
    "with open('./bygram_list/bygram.pkl','rb') as f:\n",
    "    bygram_list = pickle.load(f)\n",
    "\n",
    "#tfidfでおもみ付けしたw2vベクトル\n",
    "tfidf_sent_vectors_array = np.load('./array/tfidf_matrix_fixed.pkl.npy',allow_pickle=True)\n",
    "\n",
    "#W2V model load\n",
    "model = gensim.models.KeyedVectors.load('./model/w2v_nottrained_skip.model')\n",
    "\n",
    "#n-gram func\n",
    "def n_gram(target, n):\n",
    "  # 基準を1文字(単語)ずつ ずらしながらn文字分抜き出す\n",
    "  return [target[idx:idx+n] for idx in range(len(target) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab408667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd955317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_company_f(input_text):\n",
    "    #bm25plus\n",
    "    bm25 = BM25Plus(tok_text)\n",
    "    #クエリをトークン化\n",
    "    tokenized_query = mecab_analyzer(input_text)\n",
    "\n",
    "    #query to vector(w2vで使う)\n",
    "    text_vector = get_vector(input_text)\n",
    "    #bm25による全文のスコア算出(0-1のスケールになるように正規化)\n",
    "    bm25numerator = np.array((bm25.get_scores(tokenized_query)) - np.min(np.array(bm25.get_scores(tokenized_query))))\n",
    "    bm25denominator = (np.max(np.array(bm25.get_scores(tokenized_query))-np.min(np.array(bm25.get_scores(tokenized_query)))))\n",
    "    BM25_mm_scores = bm25numerator/bm25denominator\n",
    "    \n",
    "    \n",
    "    #文書間のcos類似度計算(検索クエリをW2V化 vs W2V行列)\n",
    "    text_similarity = cosine_similarity(text_vector.reshape(-1,200), tfidf_sent_vectors_array.reshape(-1,200)).flatten()\n",
    "    \n",
    "    #各レビューとinputtextの、bygramでの一致スコアを計測する\n",
    "    bygram_score = []\n",
    "    \n",
    "    for p in tqdm(bygram_list):\n",
    "        bygram_inclusion = 0\n",
    "        \n",
    "        for k in n_gram(tokenized_query,2):\n",
    "            bygram_inclusion += p.count(k)\n",
    "\n",
    "    \n",
    "        try:\n",
    "            bygram_score.append(bygram_inclusion/len(p))        \n",
    "        except ZeroDivisionError:\n",
    "            bygram_score.append(0)\n",
    "            \n",
    "    #bygram_scoreも正規化しておく必要がある\n",
    "    text_similarity_numerator = text_similarity - np.min(text_similarity)\n",
    "    text_similarity_denominator = np.max(text_similarity) - np.min(text_similarity)\n",
    "    text_similarity_mm_scores = text_similarity_numerator/text_similarity_denominator\n",
    "    \n",
    "    #text_similarityも\n",
    "    bygramnumerator = np.array(bygram_score) - np.min(bygram_score)\n",
    "    bygramdenominator = np.max(np.array(bygram_score)) - np.min(np.array(bygram_score))\n",
    "    bygram_mm_scores = bygramnumerator/bygramdenominator\n",
    "    \n",
    "    \n",
    "    #W2Vでの類以度 + BM25+のレビュー単位でのスコア + Bygram単位での類以度\n",
    "    #足し算して平均しているだけ\n",
    "    #bygramw2v = ((text_similarity_mm_scores) + (BM25_mm_scores) + bygram_mm_scores)/3\n",
    "    #bygramw2v = (text_similarity + BM25_mm_scores + np.array(bygram_score))/3\n",
    "    #bygramw2v = (BM25_mm_scores + np.array(bygram_score))\n",
    "    #bygramw2v = (text_similarity) + np.array((bm25.get_scores(tokenized_query))) + (bygram_score)\n",
    "    #bygramw2v = ((text_similarity_mm_scores) * (BM25_mm_scores) * (bygram_mm_scores))\n",
    "    #bygramw2v = (np.array(bm25.get_scores(tokenized_query)) * np.array(bygram_score))\n",
    "    bygramw2v = (bygram_mm_scores + 2*BM25_mm_scores)/3\n",
    "    #bygramw2v = (BM25_mm_scores + bygram_mm_scores)/2 * text_similarity_mm_scores\n",
    "    #類似度でsortとargsortして、類似度とインデックス双方を取り出す topn件\n",
    "    top=20\n",
    "    top_indices = np.argsort(-bygramw2v).tolist()[0:top+1]\n",
    "    top_similarity = np.sort(bygramw2v)[::-1].tolist()[0:top+1]\n",
    "\n",
    "    #dfのハコを作り\n",
    "    answer_df = pd.DataFrame(columns=['類似度','企業名', '評価', '本文', '職種', '経験年数','現職/退職','新卒/中途','性別'])\n",
    "    \n",
    "    #カラム内の文字数。デフォルトは50なので変更\n",
    "    pd.set_option(\"display.max_colwidth\", 1500)\n",
    "    \n",
    "    #行数上限も変更し\n",
    "    pd.set_option(\"display.max_rows\", 101)\n",
    "\n",
    "    #forループで連結する\n",
    "    for a,b in zip(top_indices,top_similarity):\n",
    "        index_data = company_df.iloc[a]\n",
    "        answer_df =answer_df.append({'類似度': b,'企業名': index_data[0], \n",
    "                                     '評価': index_data[4],'本文':index_data[5],\n",
    "                                     '職種':index_data[8],'経験年数':index_data[9],\n",
    "                                     '現職/退職':index_data[10],'新卒/中途':index_data[11],\n",
    "                                     '性別':index_data[12]},ignore_index=True)\n",
    "\n",
    "    \n",
    "    #出来上がったdfを、company_info_dfとマージ\n",
    "    answer_df_marged = pd.merge(answer_df,pd.DataFrame(company_info_df[['企業名','総合スコア']]),on='企業名',how='left')\n",
    "    \n",
    "\n",
    "    #出来上がったdfを、類以度でsort\n",
    "    answer_df_marged_g = answer_df_marged.set_index(['総合スコア','企業名'])\n",
    "\n",
    "    return answer_df_marged.sort_values(by='類似度',ascending=False)\n",
    "\n",
    "#similar_company_f('若手が活躍している')\n",
    "#similar_company_f('残業が少ない')\n",
    "\n",
    "#similar_company_f('残業が少なく働きやすい')\n",
    "similar_company_f('実力主義で殺伐としている')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62181f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram(mecab_analyzer(\"残業が少なく働きやすい\"), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fdcd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_analyzer('残業が少なく働きやすい')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef378a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_analyzer('実力主義の会社')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f911781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ginza\n",
    "nlp = spacy.load('ja_ginza') # spacyにginzaを使用することを指定\n",
    "\n",
    "txt = 'ゆるい雰囲気で人間関係も良好' # 入力文字列\n",
    "\n",
    "doc = nlp(txt) # モデルへ適応\n",
    "ginza.set_split_mode(nlp, \"C\") # 形態素の分割モード指定\n",
    "result_list = []\n",
    "for sent in doc.sents:\n",
    "    result_list = result_list + [[str(token.i), token.text, token.lemma_, token.pos_, token.tag_] for token in sent]\n",
    "    \n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade89ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1420b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
